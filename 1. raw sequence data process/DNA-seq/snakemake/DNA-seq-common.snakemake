import pandas as pd

cnv_binsize=str(config["cnv_binsize"])
wig_binsize=config["wig_binsize"]
regions=config["count_regions"].strip().split(",")
# sample_ids=open("data/{}/meta_data/sample_ids.txt".format(config["dataset"])).read().strip().split("\n")
sample_tab=pd.read_table("data/{}/meta_data/sample_table.txt".format(config["dataset"]))
# validate(sample_tab, schema="../schemas/sample_tab.schema.yaml")
sample_ids=sample_tab["sample"].unique().tolist()
sample_ids2="{"+",".join(sample_ids)+"}"
ref_sample_ids=sample_tab.loc[sample_tab["group"]==config["ref_group"],"sample"]
treat_sample_ids=sample_tab.loc[sample_tab["group"]!=config["ref_group"],"sample"]

#ref_sample_ids=sample_tab.loc[sample_tab["ref_sample"]=="Y","sample"].unique().tolist()

indir="data/{}/fastq".format(config["dataset"])
outdir="output/{}".format(config["dataset"])
dataset="{}".format(config["dataset"])
gn_size=config['gn_size']

#usual bam dir
bam_dir=outdir+"/bam" if not config["remove_duplications"] else outdir+"/bam-sorted-deduped"

#bam dir with correction
RG_dir=outdir+"/bam-sorted-RG" if not config["remove_duplications"] else outdir+"/bam-sorted-deduped-RG"
BQSR_dir=outdir+"/bam-sorted-RG-BQSR" if not config["remove_duplications"] else outdir+"/bam-sorted-deduped-RG-BQSR"
GC_dir=outdir+"/bam-sorted-RG-correctGC" if not config["remove_duplications"] else outdir+"/bam-sorted-deduped-RG-correctGC"

#tools dir/path
tmp_dir=config["tmp_dir"]
gn_dir=config["gn_dir"]
gtf_dir=config["gtf_dir"]
#bwa_dir=config['bwa_dir']
trim_galore_path=config['trim_galore_path']
bwa_index_prefix=config['bwa_index_prefix']
gn_fa_path=config['gn_fa_path']
gn_2bit_path=config['gn_2bit_path']
gn_blacklist_path=config['gn_blacklist_path']
dbSNP_path=config['dbSNP_path']
gnomad_path=config['gnomad_path']
genome1k_path=config['genome1k_path']
kraken2db_dir=config['kraken2db_dir']

def request(config,outdir,sample_ids):
    output=dict()
    # output["qc0"]=expand(outdir+"/qc0/{sample_id}/{sample_id}_1_fastqc.html",outdir=outdir,sample_id=sample_ids)
    # output["qc1"]=expand(outdir+"/qc1/{sample_id}/{sample_id}_1_fastqc.html",outdir=outdir,sample_id=sample_ids)
    if config["remove_duplications"]: 
        output["bam"]=expand(outdir+"/bam-sorted-deduped/{sample_id}.bam",outdir=outdir,sample_id=sample_ids)
    else:
        output["bam"]=expand(outdir+"/bam/{sample_id}.bam",outdir=outdir,sample_id=sample_ids)
    output["bigwig"]=expand(outdir+"/wig/{sample_id}.bigwig",outdir=outdir,sample_id=sample_ids),
    
    ## feature output & matrix
    if config["SNV"]: 
        output["SNV"]=expand(outdir+"/vcf-filtered/{sample_id}.vcf.gz",outdir=outdir,sample_id=sample_ids),
        if config["BQSR"]:
            output["BQSR"]=expand(BQSR_dir+"/{sample_id}.bam",sample_id=sample_ids), 
    if config["CNV_coverage"]: 
        #CNV option1
        output["CNV"]=expand(outdir+"/segment-coverage/{sample_id}.bed",outdir=outdir,sample_id=sample_ids),
    if config["CNV_WisecondorX"]: 
        #CNV option2
        output["wisecondorx_cnv"]=expand(outdir+"/wisecondorx/CNV/"+cnv_binsize+"/{sample_id}_segments.bed",outdir=outdir,sample_id=sample_ids),
        #output["wisecondorx_ref"]=expand(outdir+"/wisecondorx/ref/reference_nipt_"+config["cnv_binsize"]+".npz",outdir=outdir,sample_id=sample_ids),
        output["wisecondorx_gene_log2ratio"]=outdir+"/matrix/CNVlog2ratio_matrix_gene.txt",
        output["wisecondorx_gene_zscore"]=outdir+"/matrix/CNVzscore_matrix_gene.txt", 
    if config["CNV_CNVkit"]: 
        #CNV option3
        output["CNVkit_cnr"]=expand(outdir+"/CNVkit/CNV/"+cnv_binsize+"/{sample_id}.cnr",outdir=outdir,sample_id=sample_ids),
        output["CNVkit_cns"]=expand(outdir+"/CNVkit/CNV/"+cnv_binsize+"/{sample_id}.cns",outdir=outdir,sample_id=sample_ids),

    if config["correct_GC"]:
        output["correct_GC"]=expand(GC_dir+"/{sample_id}.bam",sample_id=sample_ids), 
    if config["SV_manta"]: 
        output["SV_manta"]=expand(outdir+"/struatural-variation/{sample_id}/results/variants/candidateSV.vcf.gz",outdir=outdir,sample_id=sample_ids),    
    output["fragsize"]=expand(outdir+"/fragment-length/histogram.txt",outdir=outdir),
    output["count_matrix"]=expand(outdir+"/matrix/count_matrix_{region}.txt",sample_id=sample_ids,region=regions),
    output["count_matrix_log"]=expand(outdir+"/matrix/count_matrix_{region}.txt.summary",sample_id=sample_ids,region=regions),
    #output["TPM_matrix"]=expand(outdir+"/matrix/TPM_matrix_{region}.txt",sample_id=sample_ids,region=regions),
    # output["CPM_matrix"]=expand(outdir+"/matrix/CPM_matrix_{region}.txt",sample_id=sample_ids,region=regions),
    output["CPM_TMM_matrix"]=expand(outdir+"/matrix/CPM-TMM_matrix_{region}.txt",sample_id=sample_ids,region=regions),

    ## summary stats
    output["firstAlignmentSummary"]=expand(outdir+"/firstAlignmentSummary/{sample_id}.AlignmentSummaryMetrics.txt",outdir=outdir,sample_id=sample_ids),
    output["stat"]=expand(outdir+"/stats/{sample_id}.txt",outdir=outdir,sample_id=sample_ids),
    output["flagstat"]=expand(outdir+"/flagstat/{sample_id}.txt",outdir=outdir,sample_id=sample_ids),
#    output["depth"]=expand(outdir+"/stats/coverage_depth.txt",outdir=outdir),
    output["wgs_qc"]=expand(outdir+"/wgs_qc/quality-control.txt",outdir=outdir),
    if config["microbe"]: 
        output["microbe"]=expand(outdir+"/microbe/report/{sample_id}.txt",outdir=outdir,sample_id=sample_ids),

    return list(output.values())

rule all:
    input:request(config,outdir,sample_ids)





# get masked genomic regions (remove rmsk and/or blacklit)
rule maskGnRegions:
    input:
        chromsize=gn_dir+"/chrom.size",
        rmsk=gn_dir+"/rmsk.bed",
        blacklist=gn_dir+"/hg38_blacklist.bed"
    output:
        binbed=gn_dir+"/hg38.bins."+cnv_binsize+".bed",
        binbed_normsk=gn_dir+"/hg38.bins."+cnv_binsize+".norepeats.bed",
        binbed_normsk_noblacklist=gn_dir+"/hg38.bins."+cnv_binsize+".norepeats.noblacklist.bed",
    conda:
        "envs/DNA.yml"
    params:
        bs=int(cnv_binsize),
    log: 
        outdir+"/log/buildMaskGnRegions.log"
    shell:
        """
        bedtools makewindows -g {input.chromsize} -w {params.bs} > {output.binbed}
        bedtools subtract -a {output.binbed} -b {input.rmsk} > {output.binbed_normsk}
        bedtools subtract -a {output.binbed_normsk} -b {input.blacklist} > {output.binbed_normsk_noblacklist}
        """
# for i in 300 3000 10000 100000
# do echo $i
#     bedtools makewindows -g genome/chrom.size -w ${i} > genome/hg38.bins.${i}.bed
#     bedtools subtract -a genome/hg38.bins.${i}.bed -b genome/rmsk.bed > genome/hg38.bins.${i}.norepeats.bed
#     bedtools subtract -a genome/hg38.bins.${i}.norepeats.bed -b genome/hg38_blacklist.bed > genome/hg38.bins.${i}.norepeats.noblacklist.bed
#     touch -r genome/archive/hg38.bins.300.bed genome/hg38.bins.${i}.bed 
#     touch -r genome/archive/hg38.bins.300.bed genome/hg38.bins.${i}.norepeats.bed 
#     touch -r genome/archive/hg38.bins.300.bed genome/hg38.bins.${i}.norepeats.noblacklist.bed
# done


### TODO:

# # create snakemake rule to convert gtf to bed
# rule gtf2bed:
#     input:
#         gtf=gtf_dir+"/gencode.v38.annotation.gtf"
#     output:
#         bed=gtf_dir+"/gencode.v38.annotation.bed"
#     conda:
#         "envs/DNA.yml"
#     shell:
#         """
#         bin/gtf2bed < {input.gtf} > {output.bed}
#         """


# # create snakemake rule to build bwa-mem2 index
# rule bwa_index:
#     input:
#         fa=gn_fa_path
#     output:
#         prefix=bwa_index_prefix
#     conda:
#         "envs/DNA.yml"
#     shell:
#         """
#         bwa-mem2 index {input.fa} -p {output.prefix}
#         """